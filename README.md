# ğŸš€ K8s GPU Management System

[![Spring Boot](https://img.shields.io/badge/Spring%20Boot-3.2-brightgreen.svg)](https://spring.io/projects/spring-boot)
[![Oracle](https://img.shields.io/badge/Oracle-21c-red.svg)](https://www.oracle.com/database/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.25+-blue.svg)](https://kubernetes.io/)
[![Java](https://img.shields.io/badge/Java-17-orange.svg)](https://openjdk.java.net/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **vLLM & SGLang ëª¨ë¸ ì„œë¹™ì„ ìœ„í•œ ì§€ëŠ¥í˜• GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ í”Œë«í¼**

Kubernetes í™˜ê²½ì—ì„œ ë‹¤ì–‘í•œ GPU ë¦¬ì†ŒìŠ¤(GTX 1080ë¶€í„° H100ê¹Œì§€)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , MIG(Multi-Instance GPU) ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë¹„ìš©ì„ ìµœì í™”í•˜ëŠ” í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì£¼ìš” ê¸°ëŠ¥](#-ì£¼ìš”-ê¸°ëŠ¥)
- [ì§€ì› GPU ëª¨ë¸](#-ì§€ì›-gpu-ëª¨ë¸)
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
- [ì„¤ì¹˜ ê°€ì´ë“œ](#-ì„¤ì¹˜-ê°€ì´ë“œ)
- [API ë¬¸ì„œ](#-api-ë¬¸ì„œ)
- [ëŒ€ì‹œë³´ë“œ](#-ëŒ€ì‹œë³´ë“œ)
- [ì„¤ì •](#-ì„¤ì •)
- [ëª¨ë‹ˆí„°ë§](#-ëª¨ë‹ˆí„°ë§)
- [ë¬¸ì œ í•´ê²°](#-ë¬¸ì œ-í•´ê²°)
- [ê¸°ì—¬í•˜ê¸°](#-ê¸°ì—¬í•˜ê¸°)
- [ë¼ì´ì„ ìŠ¤](#-ë¼ì´ì„ ìŠ¤)

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### ğŸ“Š ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
- **ë‹¤ì–‘í•œ GPU ëª¨ë¸** í†µí•© ê´€ë¦¬ (14ì¢… ì§€ì›)
- **ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­** ìˆ˜ì§‘ (ì‚¬ìš©ë¥ , ì˜¨ë„, ì „ë ¥, ë©”ëª¨ë¦¬)
- **MIG ì¸ìŠ¤í„´ìŠ¤** ê´€ë¦¬ (H100, A100)
- **ê³¼ì—´ ë° ì´ìƒ ìƒíƒœ** ìë™ ê°ì§€

### ğŸ® ì§€ëŠ¥í˜• ë¦¬ì†ŒìŠ¤ í• ë‹¹
- **ìë™ GPU í• ë‹¹** ì‹œìŠ¤í…œ
- **ì›Œí¬ë¡œë“œ ê¸°ë°˜** ìµœì  ë¦¬ì†ŒìŠ¤ ë§¤ì¹­
- **MIG íŒŒí‹°ì…”ë‹** ìë™ ê´€ë¦¬
- **ë¹„ìš© ê¸°ë°˜** í• ë‹¹ ìµœì í™”

### ğŸ’° ë¹„ìš© ê´€ë¦¬ ë° ìµœì í™”
- **ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì **
- **íŒ€/í”„ë¡œì íŠ¸ë³„** ë¹„ìš© ë¶„ì„
- **ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜** ìµœì í™” ì œì•ˆ
- **ì˜ˆì‚° ê´€ë¦¬** ë° ì•ŒëŒ

### ğŸ“ˆ ì˜ˆì¸¡ ë¶„ì„
- **ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œ** ë¶„ì„
- **ìš©ëŸ‰ ê³„íš** ìˆ˜ë¦½ ì§€ì›
- **ë¹„ìš© ì˜ˆì¸¡** ë° ì˜ˆì‚° ê³„íš
- **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬** ê´€ë¦¬

## ğŸ® ì§€ì› GPU ëª¨ë¸

### Gaming Series
| ëª¨ë¸ | ë©”ëª¨ë¦¬ | ì•„í‚¤í…ì²˜ | MIG ì§€ì› | ìƒíƒœ |
|------|--------|----------|----------|------|
| GTX 1080 | 8GB | Pascal | âŒ | EOL |
| GTX 1080 Ti | 11GB | Pascal | âŒ | EOL |
| Titan Xp | 12GB | Pascal | âŒ | EOL |
| RTX 2080 | 8GB | Turing | âŒ | Active |
| RTX 2080 Ti | 11GB | Turing | âŒ | Active |
| RTX 3080 | 10GB | Ampere | âŒ | Active |
| RTX 3090 | 24GB | Ampere | âŒ | Active |
| RTX 4080 | 16GB | Ada Lovelace | âŒ | Active |
| RTX 4090 | 24GB | Ada Lovelace | âŒ | Active |

### Professional/Datacenter Series
| ëª¨ë¸ | ë©”ëª¨ë¦¬ | ì•„í‚¤í…ì²˜ | MIG ì§€ì› | ìµœëŒ€ MIG ì¸ìŠ¤í„´ìŠ¤ |
|------|--------|----------|----------|-------------------|
| Tesla V100 16GB | 16GB | Volta | âŒ | - |
| Tesla V100 32GB | 32GB | Volta | âŒ | - |
| A100 PCIe 40GB | 40GB | Ampere | âœ… | 7 |
| A100 SXM4 80GB | 80GB | Ampere | âœ… | 7 |
| H100 PCIe 80GB | 80GB | Hopper | âœ… | 7 |

### MIG í”„ë¡œí•„ ì§€ì›
```
H100/A100 MIG í”„ë¡œí•„:
â”œâ”€â”€ 1g.10gb  - 1 compute slice, 10GB memory (7 instances)
â”œâ”€â”€ 2g.20gb  - 2 compute slices, 20GB memory (3 instances)  
â”œâ”€â”€ 3g.40gb  - 3 compute slices, 40GB memory (2 instances)
â””â”€â”€ 7g.80gb  - 7 compute slices, 80GB memory (1 instance)
```

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "Client Layer"
        WebUI[Web Dashboard]
        API[REST API Clients]
        Grafana[Grafana]
    end
    
    subgraph "Application Layer"
        Controller[Controllers]
        Service[Services]
        Repository[Repositories]
    end
    
    subgraph "Data Layer"
        Oracle[(Oracle Database)]
        Cache[Redis Cache]
    end
    
    subgraph "Kubernetes Cluster"
        K8sAPI[K8s API Server]
        MetricsServer[Metrics Server]
        Pods[Model Serving Pods]
        Nodes[GPU Nodes]
    end
    
    subgraph "GPU Infrastructure"
        GPU1[RTX 4090]
        GPU2[A100 80GB]
        GPU3[H100 80GB]
        MIG[MIG Instances]
    end
    
    WebUI --> Controller
    API --> Controller
    Controller --> Service
    Service --> Repository
    Repository --> Oracle
    Service --> Cache
    
    Service --> K8sAPI
    Service --> MetricsServer
    K8sAPI --> Pods
    K8sAPI --> Nodes
    
    Nodes --> GPU1
    Nodes --> GPU2
    Nodes --> GPU3
    GPU2 --> MIG
    GPU3 --> MIG
    
    Grafana --> Controller
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì „ì œ ì¡°ê±´

- Java 17+
- Spring Boot 3.2+
- Oracle Database 19c+
- Kubernetes 1.25+
- Docker & Docker Compose
- NVIDIA GPU ë“œë¼ì´ë²„
- nvidia-container-toolkit

### 1ë¶„ ë°ëª¨ ì‹¤í–‰

```bash
# 1. í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-org/k8s-gpu-management.git
cd k8s-gpu-management

# 2. Docker Composeë¡œ ì‹¤í–‰
docker-compose up -d

# 3. ì›¹ ëŒ€ì‹œë³´ë“œ ì ‘ì†
open http://localhost:8080/k8s-monitor

# 4. GPU ì •ë³´ í™•ì¸
curl http://localhost:8080/k8s-monitor/api/v1/gpu/overview
```

## ğŸ“¦ ì„¤ì¹˜ ê°€ì´ë“œ

### Option 1: Kubernetes ë°°í¬

```bash
# 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace k8s-monitoring

# 2. Oracle ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
kubectl apply -f k8s/oracle-db.yaml

# 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬
kubectl apply -f k8s/gpu-monitor.yaml

# 4. ì„œë¹„ìŠ¤ í™•ì¸
kubectl get pods -n k8s-monitoring
```

### Option 2: ë¡œì»¬ ê°œë°œ í™˜ê²½

```bash
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export SPRING_PROFILES_ACTIVE=development,gpu-management
export DB_HOST=localhost
export DB_USERNAME=gpu_admin
export DB_PASSWORD=password

# 2. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
./scripts/init-database.sh

# 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
./mvnw spring-boot:run

# 4. ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
open http://localhost:8080/k8s-monitor
```

### Option 3: Production ë°°í¬

ìì„¸í•œ ë‚´ìš©ì€ [ì„¤ì¹˜ ê°€ì´ë“œ](docs/installation.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ“š API ë¬¸ì„œ

### GPU ê´€ë¦¬ API

#### í´ëŸ¬ìŠ¤í„° ê°œìš”
```http
GET /api/v1/gpu/overview
```

```json
{
  "totalGpuDevices": 24,
  "activeAllocations": 15,
  "totalMigInstances": 42,
  "overallGpuUtilization": 67.5,
  "devicesByModel": {
    "H100_80GB": 8,
    "A100_80GB": 12,
    "RTX4090": 4
  }
}
```

#### GPU ì¥ë¹„ ì¡°íšŒ
```http
GET /api/v1/gpu/devices?nodeName=worker-01
```

#### GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹
```http
POST /api/v1/gpu/allocations
Content-Type: application/json

{
  "namespace": "model-serving",
  "podName": "vllm-llama2-7b",
  "workloadType": "Inference",
  "useMig": true,
  "requiredMemoryGb": 20
}
```

#### MIG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
```http
POST /api/v1/gpu/devices/{deviceId}/mig
Content-Type: application/json

["H100_2G20GB", "H100_3G40GB"]
```

ì „ì²´ API ë¬¸ì„œ: [API Reference](docs/api.md)

## ğŸ–¥ï¸ ëŒ€ì‹œë³´ë“œ

### ë©”ì¸ ëŒ€ì‹œë³´ë“œ
![ë©”ì¸ ëŒ€ì‹œë³´ë“œ](docs/images/dashboard-main.png)

- **ì‹¤ì‹œê°„ í´ëŸ¬ìŠ¤í„° ìƒíƒœ** ëª¨ë‹ˆí„°ë§
- **GPU ì‚¬ìš©ë¥ ** ë° **ì˜¨ë„** ì¶”ì 
- **ë¹„ìš© ë¶„ì„** ë° **ì˜ˆì‚° ê´€ë¦¬**
- **ì•ŒëŒ ë° ì´ìŠˆ** ê´€ë¦¬

### GPU ê´€ë¦¬ í™”ë©´
![GPU ê´€ë¦¬](docs/images/dashboard-gpu.png)

- **GPU ì¥ë¹„ ì¸ë²¤í† ë¦¬**
- **MIG ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬**
- **í• ë‹¹ í˜„í™©** ì¶”ì 
- **ì„±ëŠ¥ ë©”íŠ¸ë¦­** ì‹œê°í™”

### ë¹„ìš© ë¶„ì„ í™”ë©´
![ë¹„ìš© ë¶„ì„](docs/images/dashboard-cost.png)

- **íŒ€ë³„/í”„ë¡œì íŠ¸ë³„** ë¹„ìš© ë¶„ì„
- **ì‚¬ìš© íŒ¨í„´** íŠ¸ë Œë“œ
- **ìµœì í™” ì œì•ˆ**
- **ì˜ˆì‚° ì•ŒëŒ**

## âš™ï¸ ì„¤ì •

### application.yml ì„¤ì •

```yaml
# GPU ê´€ë¦¬ ì„¤ì •
gpu:
  management:
    enabled: true
    
    # ì§€ì› ëª¨ë¸
    supported-models:
      - H100_80GB
      - A100_80GB
      - RTX4090
    
    # MIG ì„¤ì •
    mig:
      enabled: true
      auto-cleanup: true
    
    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    metrics:
      collection-interval: 30s
      retention-days: 30
    
    # ë¹„ìš© ì„¤ì •
    cost:
      enabled: true
      default-rates:
        H100_80GB: 8.0
        A100_80GB: 6.0
        RTX4090: 2.0
```

### í™˜ê²½ë³„ ì„¤ì •

- **Development**: H2 ì¸ë©”ëª¨ë¦¬ DB, ëª¨ì˜ GPU ë°ì´í„°
- **Staging**: PostgreSQL, ì œí•œëœ GPU í’€
- **Production**: Oracle DB, ì „ì²´ GPU í´ëŸ¬ìŠ¤í„°

ìì„¸í•œ ì„¤ì •: [Configuration Guide](docs/configuration.md)

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Prometheus ë©”íŠ¸ë¦­

```prometheus
# GPU ì‚¬ìš©ë¥ 
gpu_utilization{device_id="worker-01-GPU-00", model="H100_80GB"} 85.2

# GPU ì˜¨ë„
gpu_temperature{device_id="worker-01-GPU-00"} 78.5

# MIG í• ë‹¹ë¥ 
mig_allocation_ratio{profile="2g.20gb"} 0.75

# ë¹„ìš© ë©”íŠ¸ë¦­
gpu_hourly_cost{team="ai-research", project="llm-training"} 48.0
```

### Grafana ëŒ€ì‹œë³´ë“œ

```bash
# Grafana ëŒ€ì‹œë³´ë“œ import
curl -X POST http://grafana:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @grafana/gpu-dashboard.json
```

### ì•ŒëŒ ì„¤ì •

```yaml
# ê³¼ì—´ ì•ŒëŒ
- alert: GPUOverheating
  expr: gpu_temperature > 85
  for: 5m
  annotations:
    summary: "GPU {{ $labels.device_id }} is overheating"

# ë†’ì€ ì‚¬ìš©ë¥  ì•ŒëŒ
- alert: HighGPUUtilization
  expr: gpu_utilization > 95
  for: 10m
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

#### 1. GPU ì¥ë¹„ ì¸ì‹ ì‹¤íŒ¨
```bash
# nvidia-smi í™•ì¸
nvidia-smi

# ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸
nvidia-container-cli info

# ê¶Œí•œ í™•ì¸
ls -la /dev/nvidia*
```

#### 2. MIG ì„¤ì • ì˜¤ë¥˜
```bash
# MIG ëª¨ë“œ í™œì„±í™”
sudo nvidia-smi -mig 1

# MIG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
sudo nvidia-smi mig -cgi 1g.5gb

# ìƒíƒœ í™•ì¸
nvidia-smi -L
```

#### 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨
```bash
# Oracle ì—°ê²° í…ŒìŠ¤íŠ¸
sqlplus gpu_admin/password@localhost:1521/ORCL

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
telnet oracle-db 1521
```

#### 4. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨
```bash
# Pod ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/k8s-gpu-monitor -n k8s-monitoring

# ë©”íŠ¸ë¦­ ì„œë²„ ìƒíƒœ
kubectl get apiservice v1beta1.metrics.k8s.io
```

ì „ì²´ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ: [Troubleshooting](docs/troubleshooting.md)

## ğŸ”’ ë³´ì•ˆ

### RBAC ì„¤ì •

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-monitor
rules:
- apiGroups: [""]
  resources: ["nodes", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["nodes", "pods"]
  verbs: ["get", "list"]
```

### ë°ì´í„° ë³´ì•ˆ

- **ì•”í˜¸í™”ëœ ë°ì´í„°ë² ì´ìŠ¤** ì—°ê²°
- **API í‚¤ ê¸°ë°˜** ì¸ì¦
- **ê°ì‚¬ ë¡œê¹…** í™œì„±í™”
- **ë„¤íŠ¸ì›Œí¬ ì •ì±…** ì ìš©

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ë°ì´í„°ë² ì´ìŠ¤ íŠœë‹

```sql
-- ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX idx_gpu_metrics_device_time 
ON gpu_usage_metrics(device_id, timestamp);

-- íŒŒí‹°ì…”ë‹
ALTER TABLE gpu_usage_metrics 
PARTITION BY RANGE (timestamp)
INTERVAL(NUMTOYMINTERVAL(1, 'MONTH'));
```

### ìºì‹œ ì „ëµ

```java
@Cacheable(value = "gpuDevices", key = "#nodeName")
public List<GpuDeviceInfo> getGpuDevicesByNode(String nodeName) {
    // êµ¬í˜„...
}
```

### ë°°ì¹˜ ì²˜ë¦¬

```yaml
gpu:
  management:
    metrics:
      batch-size: 500
      parallel-processing: true
```
